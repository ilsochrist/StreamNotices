ğŸš€ Data Lakehouse EscalÃ¡vel e Seguro (Streaming & Medallion Architecture)ğŸŒŸ VisÃ£o Geral do ProjetoEste projeto demonstra a construÃ§Ã£o de uma Arquitetura Data Lakehouse de ponta a ponta, escalÃ¡vel e segura, projetada para processar dados de streaming em tempo (quase) real.O pipeline utiliza a Arquitetura Medallion (Bronze, Silver, Gold) para garantir a integridade, rastreabilidade e qualidade dos dados, culminando em insights acionÃ¡veis para anÃ¡lise preditiva e Business Intelligence (BI).ğŸ’¡ Stack TecnolÃ³gicoCategoriaFerramentaUso no ProjetoPlataforma/ComputaÃ§Ã£oDatabricksDesenvolvimento e execuÃ§Ã£o de Jobs de ETL/ELT e orquestraÃ§Ã£o.Data LakehouseDelta LakeArmazenamento de dados, garantindo confiabilidade, transaÃ§Ãµes ACID e performance otimizada.LinguagemPython / SQLDesenvolvimento dos Jobs de processamento.SeguranÃ§aVariÃ¡veis de Ambiente / Secret ManagementAutenticaÃ§Ã£o segura do workspace e isolamento de credenciais.VisualizaÃ§Ã£oPower BIConsumo da camada Gold para criaÃ§Ã£o de dashboards de negÃ³cio.AnÃ¡lise AvanÃ§adaModelos de Text AnalysisGeraÃ§Ã£o de scores de sentimento.ğŸ—ï¸ Arquitetura Medallion em DetalhesO projeto implementa rigorosamente o padrÃ£o Medallion para garantir um fluxo de dados limpo e confiÃ¡vel:ğŸ¥‰ Camada Bronze (Raw)FunÃ§Ã£o: IngestÃ£o de dados brutos e imutÃ¡veis (raw data), mantendo a fonte original.CaracterÃ­sticas: Dados com formato e tipo originais da fonte (streaming).ğŸ¥ˆ Camada Silver (Cleaned & Enriched)FunÃ§Ã£o: Limpeza, padronizaÃ§Ã£o e enriquecimento dos dados brutos.Regras: AplicaÃ§Ã£o de regras de qualidade de dados (remoÃ§Ã£o de duplicatas, tratamento de nulos).Enriquecimento: InclusÃ£o de AnÃ¡lise de Sentimento (scores) via modelos de Text Analysis no pipeline de processamento.ğŸ¥‡ Camada Gold (Curated & Consumable)FunÃ§Ã£o: Dados Curados, modelados (ex: Star Schema) e otimizados para consumo de BI/ML.CaracterÃ­sticas: Camada final de consumo, exposta ao Power BI para geraÃ§Ã£o de relatÃ³rios e anÃ¡lises preditivas.ğŸ›¡ï¸ Foco em SeguranÃ§aA seguranÃ§a e o gerenciamento de credenciais foram priorizados:AutenticaÃ§Ã£o Segura: UtilizaÃ§Ã£o de variÃ¡veis de ambiente (env key) para autenticar o workspace do Databricks.Melhores PrÃ¡ticas: Acesso a recursos externos Ã© feito via gerenciamento de segredos para garantir o isolamento de credenciais e evitar hardcoding.ğŸ“ˆ Insights e MÃ©tricas ChaveO pipeline gera informaÃ§Ãµes de alto valor, focando na saÃºde do dado e em insights de negÃ³cio:Monitoramento de Qualidade: CriaÃ§Ã£o e rastreamento de mÃ©tricas como a LatÃªncia de IngestÃ£o, garantindo a pontualidade dos dados de streaming.AnÃ¡lise de TendÃªncias: ExtraÃ§Ã£o e persistÃªncia de Top Keywords para identificar rapidamente os tÃ³picos mais relevantes nos dados de streaming.AnÃ¡lise Preditiva: UtilizaÃ§Ã£o dos scores de Sentimento gerados na camada Silver para permitir a anÃ¡lise de tendÃªncias de humor do usuÃ¡rio em tempo (quase) real.ğŸ”— ConclusÃ£oEste projeto demonstra a capacidade de construir e gerenciar uma arquitetura Data Lakehouse complexa, garantindo confiabilidade (Delta Lake), escalabilidade (Databricks) e inteligÃªncia analÃ­tica (Sentimento) para suportar decisÃµes de negÃ³cio baseadas em dados de alta qualidade e pontualidade.
